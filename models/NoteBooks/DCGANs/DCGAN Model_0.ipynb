{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"b-Oqgzvbkons","executionInfo":{"status":"ok","timestamp":1677104303453,"user_tz":-60,"elapsed":7375,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"}}},"outputs":[],"source":["import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","from tqdm import tqdm #progress bar\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from torchvision.utils import save_image\n","\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QY09IBl8OAco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677104322473,"user_tz":-60,"elapsed":19023,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"}},"outputId":"62e7276e-a3f1-4f0f-d00b-7d68461bb0a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giop--6tkont"},"outputs":[],"source":["train_dir = '/content/drive/MyDrive/Dataset/Celeb/celebData/train'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1WYA3pUkonu"},"outputs":[],"source":["# Defining the hyper-parameters \n","\n","# Batch Size for the dataset\n","BATCH_SIZE = 128\n","\n","# setting image sizes for our dataset \n","# normally 178*218 however we will reduce it to 64 for the sake of trainning\n","IMG_SIZE = 64\n","\n","# setting the number of workers\n","WORKERS = 2\n","\n","# number of channels for images in our case is 3 colors (RGB)\n","CHANNELS_NUMBER = 3\n","\n","# setting the size of the feature maps for generator\n","gen_feature = 64\n","\n","# setting the size of the feature maps for discriminator\n","dis_feature = 64\n","\n","# setting the number of epochs\n","EPOCH_NUMBER = 10\n","\n","# setting the learning rate \n","lr = 0.0002"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vUpBXk_konu"},"outputs":[],"source":["class FacesDataset(Dataset):\n","    def __init__(self, img_dir, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        #self.target_transform = target_transform\n","        self.label = 1  #because there are only good images in this dataset\n","\n","    def __len__(self):\n","        return len(os.listdir(self.img_dir))\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, os.listdir(self.img_dir)[idx])\n","        image = read_image(img_path)\n","            \n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kkaE-n3konv"},"outputs":[],"source":["transform=transforms.Compose([\n","                               transforms.Resize(IMG_SIZE),\n","                               transforms.CenterCrop(IMG_SIZE),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRnX3hGckonv","colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"status":"error","timestamp":1677011308763,"user_tz":-60,"elapsed":80806,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"}},"outputId":"beb23c4b-72d5-47a8-eed9-bc8ca212db00"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-488305f5f570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset = dset.ImageFolder(root=train_dir,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            transform=transform)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n\u001b[1;32m      5\u001b[0m     shuffle=True, pin_memory=True,drop_last=True)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes target_dir. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"]}],"source":["train_dataset = dset.ImageFolder(root=train_dir,\n","                           transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n","    shuffle=True, pin_memory=True,drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGaZwcFHkonv"},"outputs":[],"source":["# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLhNYa0akonv"},"outputs":[],"source":["real_batch = next(iter(train_loader))\n","plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tIyzI2hkonw"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            #input 100 * 1024 * 3\n","\n","            nn.ConvTranspose2d(100, 1024,kernel_size= 4, stride= 2, padding=1, bias=False),\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(True),\n","            #output  3 * 3\n","\n","            nn.ConvTranspose2d(1024, 512, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            #output  7*7\n","\n","            nn.ConvTranspose2d(512, 256, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            #output  14 * 14\n","\n","            nn.ConvTranspose2d(256, 128, kernel_size= 4, stride=2,padding= 1,bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            # output  28*28\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(64, 3, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.Tanh()\n","            )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lurVjjGOkonx"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(3, 64,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(64, 64 * 2,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(64 * 2, 64 * 4, kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(64 * 4, 64 * 8,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(64 * 8, 1,  kernel_size= 4, stride=1,padding= 0, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-M7xNxtkonx"},"outputs":[],"source":["# Create an instance of generator \n","gen_net = Generator().to(device)\n","\n","if (device.type == 'cuda'):\n","    gen_net = nn.DataParallel(gen_net, list(range(1)))\n","\n","print(gen_net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcDPKMnxkonx"},"outputs":[],"source":["# Create the Discriminator\n","dis_net = Discriminator().to(device)\n","\n","# Handle multi-gpu if desired\n","if (device.type == 'cuda'):\n","    dis_net = nn.DataParallel(dis_net, list(range(1)))\n","\n","# Print the model\n","print(dis_net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIohXwcJkony"},"outputs":[],"source":["# define loss function\n","loss_fn = nn.BCELoss()\n","\n","beta1 = 0.5\n","# Setup Adam optimizers for both G and D\n","dis_optimizer = optim.Adam(dis_net.parameters(), lr=lr, betas=(beta1, 0.999))\n","gen_optimizer = optim.Adam(gen_net.parameters(), lr=lr, betas=(beta1, 0.999))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghJOxuiPkony"},"outputs":[],"source":["fake_list = []\n","gen_losses = []\n","dis_losses =[]\n","# Train the GAN\n","for epoch in tqdm(range(EPOCH_NUMBER)):\n","    for i, (X_batch, _) in enumerate(train_loader):\n","        # Move the images and labels to the device\n","        X_batch = X_batch.to(device)\n","\n","        # Train the discriminator on real images\n","        dis_optimizer.zero_grad()\n","        # check the torch img dimensions and set labels accordingly\n","        real_labels = torch.ones(128, 1, 1, 1).to(device)\n","        fake_labels = torch.zeros(128, 1, 1, 1).to(device)\n","\n","        real_outputs = dis_net(X_batch)\n","        real_loss = loss_fn(real_outputs, real_labels)\n","        real_loss.backward()\n","        dis_optimizer.step()\n","\n","        # Train the discriminator on fake images\n","        noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n","        fake_images = gen_net(noise)\n","        fake_outputs = dis_net(fake_images.detach())\n","        fake_loss = loss_fn(fake_outputs, fake_labels)\n","        fake_loss.backward()\n","        dis_optimizer.step()\n","\n","        # save the loss for discriminator\n","        dis_losses.append((real_loss.item() + fake_loss.item()))\n","\n","        # Train the generator\n","        gen_optimizer.zero_grad()\n","        noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n","        fake_images = gen_net(noise)\n","        fake_outputs = dis_net(fake_images)\n","        gen_loss = loss_fn(fake_outputs, real_labels)\n","        gen_losses.append(gen_loss)\n","        gen_loss.backward()\n","        gen_optimizer.step()\n","   \n","    save_image(fake_images, \"/content/drive/MyDrive/Dataset/Fake_Images_new/{0:0=6d}.png\" .format(epoch+1), nrow=5, padding=2,normalize=True)\n","    x_fake = fake_images.detach().cpu().numpy()\n","\n","    for k in range(EPOCH_NUMBER):\n","        plt.subplot(2, 5, k+1)\n","        plt.imshow(x_fake[k].reshape(64,64,3))\n","       #  * 255).astype(np.uint8))\n","        plt.xticks([])\n","        plt.yticks([])\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    print(\"Epoch: %d, D Loss: %.4f, G Loss: %.4f\" % (epoch+1, real_loss.item() + fake_loss.item(), gen_loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1Mp_aOWkony"},"outputs":[],"source":["plt.imshow(np.transpose(vutils.make_grid(fake_images[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQwPhv2xkonz"},"outputs":[],"source":["ge = torch.tensor(gen_losses).long()\n","de = torch.tensor(dis_losses).long()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWwiy_R5konz"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training \"+ str(EPOCH_NUMBER))\n","plt.plot(ge,label=\"Generator\")\n","plt.plot(de,label=\"Discriminator\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.savefig('Losses During Training with '+str(EPOCH_NUMBER)+'epochs')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZE5QYJhmOMf"},"outputs":[],"source":["torch.save(dis_net.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NN PROJECT/Weights/discnewepoch.pt\")\n","torch.save(gen_net.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NN PROJECT/Weights/gennewepoch.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITTEQ1pfnS6b"},"outputs":[],"source":["torch.save(gen_net, '/content/drive/MyDrive/Colab Notebooks/NN PROJECT/models/base_gen_new.pt')\n","torch.save(dis_net, '/content/drive/MyDrive/Colab Notebooks/NN PROJECT/models/base_dis_new.pt')"]},{"cell_type":"code","source":[],"metadata":{"id":"GGbQwyM7qCj-"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rG0DqcWAA8KUhT_h92H6ZIoylSG9AO-q","timestamp":1676640757726}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"vscode":{"interpreter":{"hash":"69831eeb97e384214a5f95ae12d254e719da40722d4e8edb8de13c454c254d6e"}}},"nbformat":4,"nbformat_minor":0}