{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Importing nessesarry libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7375,"status":"ok","timestamp":1677104303453,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"},"user_tz":-60},"id":"b-Oqgzvbkons"},"outputs":[],"source":["import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","from tqdm import tqdm #progress bar\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision.io import read_image\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from torchvision.utils import save_image\n","\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19023,"status":"ok","timestamp":1677104322473,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"},"user_tz":-60},"id":"QY09IBl8OAco","outputId":"62e7276e-a3f1-4f0f-d00b-7d68461bb0a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mounting Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giop--6tkont"},"outputs":[],"source":["# Setting train dataset location\n","train_dir = '/content/drive/MyDrive/Dataset/Celeb/celebData/train'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1WYA3pUkonu"},"outputs":[],"source":["# Defining the hyper-parameters \n","\n","# Batch Size for the dataset\n","BATCH_SIZE = 128\n","\n","# setting image sizes for our dataset \n","# normally 178*218 however we will reduce it to 64 for the sake of trainning\n","IMG_SIZE = 64\n","\n","# setting the number of workers\n","WORKERS = 2\n","\n","# number of channels for images in our case is 3 colors (RGB)\n","CHANNELS_NUMBER = 3\n","\n","# setting the size of the feature maps for generator\n","gen_feature = 64\n","\n","# setting the size of the feature maps for discriminator\n","dis_feature = 64\n","\n","# setting the number of epochs\n","EPOCH_NUMBER = 10\n","\n","# setting the learning rate \n","lr = 0.0002"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Resize: This resizes the image to a square of size IMG_SIZE while maintaining the aspect ratio of the original image.\n","\n","CenterCrop: This crops the center of the image to a square of size IMG_SIZE.\n","\n","ToTensor: This converts the image to a PyTorch tensor.\n","\n","Normalize: This normalizes the tensor image with mean and standard deviation (0.5, 0.5, 0.5) for each color channel. The values are normalized to be between -1 and 1.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kkaE-n3konv"},"outputs":[],"source":["transform=transforms.Compose([\n","                               transforms.Resize(IMG_SIZE),\n","                               transforms.CenterCrop(IMG_SIZE),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The train_dataset variable is defined using the ImageFolder class from the torchvision.datasets module. This class assumes that the data is organized in a directory structure such that each subdirectory corresponds to a different class, and the images within each subdirectory belong to that class. The root argument specifies the path to the directory containing the training data, and the transform argument specifies the image transformations to be applied to the data. Here we actually do not need to have any labels since it is unsupervised learning.\n","\n","The train_loader variable is defined using the DataLoader class from the torch.utils.data module. This class provides an iterable over the dataset, allowing the model to access the training data in batches during training. The batch_size argument specifies the number of samples in each batch, shuffle argument specifies whether to shuffle the data between epochs, pin_memory argument allows for faster transfer of data to the GPU, and drop_last argument drops the last batch of data if it's smaller than the batch size.\n","\n","Overall, this code sets up a DataLoader for the training data with batch size BATCH_SIZE and image transformations transform. This DataLoader can be used to train a deep learning model on the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":80806,"status":"error","timestamp":1677011308763,"user":{"displayName":"Gökberk Keptiğ","userId":"03481023652925412504"},"user_tz":-60},"id":"ZRnX3hGckonv","outputId":"beb23c4b-72d5-47a8-eed9-bc8ca212db00"},"outputs":[],"source":["train_dataset = dset.ImageFolder(root=train_dir,\n","                           transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n","    shuffle=True, pin_memory=True,drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGaZwcFHkonv"},"outputs":[],"source":["# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This code generates a grid of sample images from the training data using the make_grid function provided by the torchvision.utils module. The real_batch variable is obtained by calling next(iter(train_loader)), which returns the next batch of data from the train_loader object.\n","\n","The make_grid function takes a tensor of images and arranges them in a grid with a specified padding. In this case, it takes the first 64 images from real_batch and applies a padding of 2 pixels between each image. The normalize=True argument normalizes the image data to the range [0, 1]. Finally, the cpu() method is called to move the tensor to the CPU if it was previously on the GPU.\n","\n","The resulting grid of images is displayed using matplotlib's imshow function. The figure function creates a new figure with a size of 8x8 inches, and the axis function is called with \"off\" to remove the axis labels. The title function sets the title of the plot to \"Training Images\".\n","\n","Overall, this code is useful for visualizing a sample of the training data to get an idea of what kind of images the model will be trained on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLhNYa0akonv"},"outputs":[],"source":["real_batch = next(iter(train_loader))\n","plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is a PyTorch implementation of a Generative Adversarial Network (GAN) generator model. The generator takes a 100-dimensional noise vector as input and outputs a 3-channel image of size 64x64.\n","\n","The model architecture consists of a series of transposed convolutional layers, with each layer gradually increasing the spatial size of the output and reducing the number of channels. Batch normalization is applied to each layer to improve training stability. The final layer uses the hyperbolic tangent activation function to scale the pixel values of the output to the range [-1, 1], which is common for image data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tIyzI2hkonw"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.main = nn.Sequential(\n","\n","            nn.ConvTranspose2d(100, 1024,kernel_size= 4, stride= 2, padding=1, bias=False),\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(1024, 512, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(512, 256, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(256, 128, kernel_size= 4, stride=2,padding= 1,bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(128, 64, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(64, 3, kernel_size= 4, stride=2,padding= 1, bias=False),\n","            nn.Tanh()\n","            )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This code defines a Discriminator class that inherits from the nn.Module class of PyTorch. It contains a main sequential model that consists of several layers that process an input image and output a scalar value indicating whether the input is real or fake.\n","\n","The layers in the main model are as follows:\n","\n","-Conv2d layer with 3 input channels and 64 output channels, using a kernel size of 4, stride of 2, and padding of 1. It applies a leaky ReLU activation function with a negative slope of 0.2 to the output.\n","\n","-Conv2d layer with 64 input channels and 128 output channels, using a kernel size of 4, stride of 2, and padding of 1. It applies batch normalization and a leaky ReLU activation function with a negative slope of 0.2 to the output.\n","\n","-Conv2d layer with 128 input channels and 256 output channels, using a kernel size of 4, stride of 2, and padding of 1. It applies batch normalization and a leaky ReLU activation function with a negative slope of 0.2 to the output.\n","\n","-Conv2d layer with 256 input channels and 512 output channels, using a kernel size of 4, stride of 2, and padding of 1. It applies batch normalization and a leaky ReLU activation function with a negative slope of 0.2 to the output.\n","\n","-Conv2d layer with 512 input channels and 1 output channel, using a kernel size of 4, stride of 1, and padding of 0. It applies a sigmoid activation function to the output, which is the final prediction of the discriminator.\n","\n","-The input to the Discriminator is expected to be a 3-channel image with a size of 64x64 pixels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lurVjjGOkonx"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            # input is (nc) x 64 x 64\n","            nn.Conv2d(3, 64,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf) x 32 x 32\n","            nn.Conv2d(64, 64 * 2,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*2) x 16 x 16\n","            nn.Conv2d(64 * 2, 64 * 4, kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*4) x 8 x 8\n","            nn.Conv2d(64 * 4, 64 * 8,  kernel_size= 4, stride=2,padding= 1, bias=True),\n","            nn.BatchNorm2d(64 * 8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # state size. (ndf*8) x 4 x 4\n","            nn.Conv2d(64 * 8, 1,  kernel_size= 4, stride=1,padding= 0, bias=True),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-M7xNxtkonx"},"outputs":[],"source":["# Create an instance of generator \n","gen_net = Generator().to(device)\n","\n","\"\"\"DataParallel is a PyTorch module that allows the parallelization of model training on multiple GPUs.\n","It divides the data into smaller batches and distributes the computation across multiple GPUs.\n","By doing this, it can significantly speed up the training process and improve model performance.\n","\n","The second argument of the DataParallel constructor is a list of device IDs that specify which GPUs to use. \n","In this case, which means the model will be parallelized on GPU 0.\n","In summary, this line of code sets up a DataParallel model that parallelizes the training of gen_net on GPU 0 if the device type is CUDA.\"\"\"\n","if (device.type == 'cuda'):\n","    gen_net = nn.DataParallel(gen_net, [0])\n","\n","# Printing the Generator model\n","print(gen_net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcDPKMnxkonx"},"outputs":[],"source":["# Create the Discriminator\n","dis_net = Discriminator().to(device)\n","\n","# Handle multi-gpu if desired\n","if (device.type == 'cuda'):\n","    dis_net = nn.DataParallel(dis_net, [0])\n","\n","# Print the model\n","print(dis_net)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The loss function is defined as Binary Cross-Entropy (BCE) loss, which is commonly used for binary classification problems, such as distinguishing real images from generated ones in GANs.\n","\n","The variable beta1 is a hyperparameter used in the Adam optimizer, which controls the decay rate of the first moment estimate of the gradient.\n","\n","Two Adam optimizers are created, one for the discriminator (dis_optimizer) and one for the generator (gen_optimizer). The parameters of each network are passed to their respective optimizer, along with the learning rate and beta1 value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIohXwcJkony"},"outputs":[],"source":["# define loss function\n","loss_fn = nn.BCELoss()\n","\n","beta1 = 0.5\n","# Setup Adam optimizers for both G and D\n","dis_optimizer = optim.Adam(dis_net.parameters(), lr=lr, betas=(beta1, 0.999))\n","gen_optimizer = optim.Adam(gen_net.parameters(), lr=lr, betas=(beta1, 0.999))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghJOxuiPkony"},"outputs":[],"source":["fake_list = []\n","gen_losses = []\n","dis_losses =[]\n","# Train the GAN\n","for epoch in tqdm(range(EPOCH_NUMBER)):\n","    for i, (X_batch, _) in enumerate(train_loader):\n","        # Move the images and labels to the device\n","        X_batch = X_batch.to(device)\n","\n","        # Train the discriminator on real images\n","        dis_optimizer.zero_grad()\n","        # check the torch img dimensions and set labels accordingly\n","        real_labels = torch.ones(128, 1, 1, 1).to(device)\n","        fake_labels = torch.zeros(128, 1, 1, 1).to(device)\n","\n","        real_outputs = dis_net(X_batch)\n","        real_loss = loss_fn(real_outputs, real_labels)\n","        real_loss.backward()\n","        dis_optimizer.step()\n","\n","        # Train the discriminator on fake images\n","        noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n","        fake_images = gen_net(noise)\n","        fake_outputs = dis_net(fake_images.detach())\n","        fake_loss = loss_fn(fake_outputs, fake_labels)\n","        fake_loss.backward()\n","        dis_optimizer.step()\n","\n","        # save the loss for discriminator\n","        dis_losses.append((real_loss.item() + fake_loss.item()))\n","\n","        # Train the generator\n","        gen_optimizer.zero_grad()\n","        noise = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n","        fake_images = gen_net(noise)\n","        fake_outputs = dis_net(fake_images)\n","        gen_loss = loss_fn(fake_outputs, real_labels)\n","        gen_losses.append(gen_loss)\n","        gen_loss.backward()\n","        gen_optimizer.step()\n","   \n","    save_image(fake_images, \"/content/drive/MyDrive/Dataset/Fake_Images_new/{0:0=6d}.png\" .format(epoch+1), nrow=5, padding=2,normalize=True)\n","    x_fake = fake_images.detach().cpu().numpy()\n","\n","    for k in range(EPOCH_NUMBER):\n","        plt.subplot(2, 5, k+1)\n","        plt.imshow(x_fake[k].reshape(64,64,3))\n","       #  * 255).astype(np.uint8))\n","        plt.xticks([])\n","        plt.yticks([])\n","\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    print(\"Epoch: %d, D Loss: %.4f, G Loss: %.4f\" % (epoch+1, real_loss.item() + fake_loss.item(), gen_loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1Mp_aOWkony"},"outputs":[],"source":["plt.imshow(np.transpose(vutils.make_grid(fake_images[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQwPhv2xkonz"},"outputs":[],"source":["ge = torch.tensor(gen_losses).long()\n","de = torch.tensor(dis_losses).long()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWwiy_R5konz"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Generator and Discriminator Loss During Training \"+ str(EPOCH_NUMBER))\n","plt.plot(ge,label=\"Generator\")\n","plt.plot(de,label=\"Discriminator\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.savefig('Losses During Training with '+str(EPOCH_NUMBER)+'epochs')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZE5QYJhmOMf"},"outputs":[],"source":["torch.save(dis_net.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NN PROJECT/Weights/discnewepoch.pt\")\n","torch.save(gen_net.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/NN PROJECT/Weights/gennewepoch.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITTEQ1pfnS6b"},"outputs":[],"source":["torch.save(gen_net, '/content/drive/MyDrive/Colab Notebooks/NN PROJECT/models/base_gen_new.pt')\n","torch.save(dis_net, '/content/drive/MyDrive/Colab Notebooks/NN PROJECT/models/base_dis_new.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGbQwyM7qCj-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rG0DqcWAA8KUhT_h92H6ZIoylSG9AO-q","timestamp":1676640757726}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"0ce1b4f8dd00291575acaea574a6fdb6486b6ed960aebf22b75a21b9d034dcf9"}}},"nbformat":4,"nbformat_minor":0}
